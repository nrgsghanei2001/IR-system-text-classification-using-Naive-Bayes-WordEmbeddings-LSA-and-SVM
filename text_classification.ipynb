{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing function:\n",
    "\n",
    "This function gets a list of texts and tokenize each text, remove stopwords and do stemming on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def tokenize_filtering(list_of_contexts):\n",
    "    all_tokens = []         # a list to save all of tokens of all documents\n",
    "    for i, doc in enumerate(list_of_contexts):\n",
    "        lower_doc = doc.lower()              # make all of contexts lower case\n",
    "        list_of_contexts[i] = lower_doc          \n",
    "        tokens = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', list_of_contexts[i])   # tokenize the text with regex\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    for i, doc in enumerate(all_tokens):\n",
    "        new_tokens = []\n",
    "        for token in doc:     # delete all of stopwords and single character tokens except numbers from token list\n",
    "            if (len(token) < 2 and token.isalpha()) or (token in stop_words):          \n",
    "                continue\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        stemmer = nltk.stem.PorterStemmer()                      # stemming each token\n",
    "        new_tokens = [stemmer.stem(token) for token in new_tokens]\n",
    "        all_tokens[i] = new_tokens\n",
    "\n",
    "    return all_tokens       # return a 2D array with contains lists of tokens of each document\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_proccessing(dataset):    \n",
    "\n",
    "    list_of_contexts = []      # all of file's contexts\n",
    "    for doc in dataset:                            # do for each document in a block\n",
    "        context = \"\" \n",
    "        with open(doc, 'r', encoding='cp437') as f:  \n",
    "            context = f.read()      # reading all files\n",
    "            list_of_contexts.append(context)\n",
    "\n",
    "    all_tokens = tokenize_filtering(list_of_contexts)          # list of all tokens in the dataset\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save address of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_addresses(path):\n",
    "    address_list = []\n",
    "    os.chdir(path) \n",
    "    for file in os.listdir():                # save the file passes in a list\n",
    "        file_path = f\"{path}\\{file}\"\n",
    "        address_list.append(file_path)\n",
    "    \n",
    "    return address_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find unique terms in given documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_terms(pos_tokens, neg_tokens):\n",
    "    vocab = {}       # a dictionary that maps each term with its frequency (TF)\n",
    "    voc_list = []    # list of all unique terms for ID mapping\n",
    "\n",
    "    for doc in pos_tokens:\n",
    "        for w in doc:\n",
    "            if w not in vocab:\n",
    "                # the value of each element in vocabulary dictionary is a list with 2 numebrs.\n",
    "                # the first number is TF in positive data and the second one is TF in negative data\n",
    "                vocab[w] = [1, 0]  \n",
    "                voc_list.append(w)   # add new word \n",
    "            else:\n",
    "                vocab[w][0] += 1   # add counter of the word\n",
    "\n",
    "    for doc in neg_tokens:\n",
    "        for w in doc:\n",
    "            if w not in vocab:\n",
    "                vocab[w]  = [0, 1]\n",
    "                voc_list.append(w)\n",
    "            else:\n",
    "                vocab[w][1] += 1\n",
    "\n",
    "    return vocab, voc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_pos = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\pos\" \n",
    "path_train_neg = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\neg\" \n",
    "path_test_pos = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\test\\\\pos\" \n",
    "path_test_neg = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\test\\\\neg\" \n",
    "\n",
    "list_of_train_pos_address = get_files_addresses(path_train_pos)  # list of address of positive training docs\n",
    "list_of_train_neg_address = get_files_addresses(path_train_neg)  # list of address of negative training docs\n",
    "list_of_test_pos_address = get_files_addresses(path_test_pos)  # list of address of positive test docs\n",
    "list_of_test_neg_address = get_files_addresses(path_test_neg)  # list of address of negative test docs\n",
    "\n",
    "# get the token list of each class\n",
    "pos_tokens = dataset_proccessing(list_of_train_pos_address)\n",
    "neg_tokens = dataset_proccessing(list_of_train_neg_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, voc_list = find_unique_terms(pos_tokens, neg_tokens)  # find the vocabulary of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{w\\in V}^{}\\frac{TF_{w,c}+\\alpha}{L_{c}+B}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(pos_tokens, neg_tokens, vocab, alpha=0.01):\n",
    "    train_matrix = {}     \n",
    "    B = alpha * len(vocab)  \n",
    "    len_pos = 0\n",
    "    for doc in pos_tokens:\n",
    "        len_pos += len(doc)\n",
    "    len_neg = 0\n",
    "    for doc in neg_tokens:\n",
    "        len_neg += len(doc)\n",
    "\n",
    "    len_pos += B   # length of each class documents\n",
    "    len_neg += B\n",
    "    \n",
    "    for i, w in enumerate(vocab):\n",
    "        count = vocab[w][0]\n",
    "        train_matrix[w] = []\n",
    "        train_matrix[w].append((count + alpha)/len_pos)   # calculate the formula for each term and save it in matrix\n",
    "        count = vocab[w][1]\n",
    "        train_matrix[w].append((count + alpha)/len_neg)\n",
    "    \n",
    "    return train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = train_naive_bayes(pos_tokens, neg_tokens, vocab)    # train the dataset with naive bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
