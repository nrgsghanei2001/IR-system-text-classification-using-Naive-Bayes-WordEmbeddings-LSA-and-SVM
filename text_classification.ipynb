{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing function:\n",
    "\n",
    "This function gets a list of texts and tokenize each text, remove stopwords and do stemming on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def tokenize_filtering(list_of_contexts):\n",
    "    all_tokens = []         # a list to save all of tokens of all documents\n",
    "    for i, doc in enumerate(list_of_contexts):\n",
    "        lower_doc = doc.lower()              # make all of contexts lower case\n",
    "        list_of_contexts[i] = lower_doc          \n",
    "        tokens = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', list_of_contexts[i])   # tokenize the text with regex\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    for i, doc in enumerate(all_tokens):\n",
    "        new_tokens = []\n",
    "        for token in doc:     # delete all of stopwords and single character tokens except numbers from token list\n",
    "            if (len(token) < 2 and token.isalpha()) or (token in stop_words):          \n",
    "                continue\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        stemmer = nltk.stem.PorterStemmer()                      # stemming each token\n",
    "        new_tokens = [stemmer.stem(token) for token in new_tokens]\n",
    "        all_tokens[i] = new_tokens\n",
    "\n",
    "    return all_tokens       # return a 2D array with contains lists of tokens of each document\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
