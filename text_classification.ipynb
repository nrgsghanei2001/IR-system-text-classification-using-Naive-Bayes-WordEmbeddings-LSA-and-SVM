{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing function:\n",
    "\n",
    "This function gets a list of texts and tokenize each text, remove stopwords and do stemming on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def tokenize_filtering(list_of_contexts):\n",
    "    all_tokens = []         # a list to save all of tokens of all documents\n",
    "    for i, doc in enumerate(list_of_contexts):\n",
    "        lower_doc = doc.lower()              # make all of contexts lower case\n",
    "        list_of_contexts[i] = lower_doc          \n",
    "        tokens = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', list_of_contexts[i])   # tokenize the text with regex\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    for i, doc in enumerate(all_tokens):\n",
    "        new_tokens = []\n",
    "        for token in doc:     # delete all of stopwords and single character tokens except numbers from token list\n",
    "            if (len(token) < 2 and token.isalpha()) or (token in stop_words):          \n",
    "                continue\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        stemmer = nltk.stem.PorterStemmer()                      # stemming each token\n",
    "        new_tokens = [stemmer.stem(token) for token in new_tokens]\n",
    "        all_tokens[i] = new_tokens\n",
    "\n",
    "    return all_tokens       # return a 2D array with contains lists of tokens of each document\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_proccessing(dataset):    \n",
    "\n",
    "    list_of_contexts = []      # all of file's contexts\n",
    "    for doc in dataset:                            # do for each document in a block\n",
    "        context = \"\" \n",
    "        with open(doc, 'r', encoding='cp437') as f:  \n",
    "            context = f.read()      # reading all files\n",
    "            list_of_contexts.append(context)\n",
    "\n",
    "    all_tokens = tokenize_filtering(list_of_contexts)          # list of all tokens in the dataset\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save address of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_addresses(path):\n",
    "    address_list = []\n",
    "    os.chdir(path) \n",
    "    for file in os.listdir():                # save the file passes in a list\n",
    "        file_path = f\"{path}\\{file}\"\n",
    "        address_list.append(file_path)\n",
    "    \n",
    "    return address_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_pos = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\pos\" \n",
    "path_train_neg = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\train\\\\neg\" \n",
    "path_test_pos = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\test\\\\pos\" \n",
    "path_test_neg = \"C:\\\\Users\\\\ASC\\\\OneDrive\\\\Desktop\\\\temp\\\\aclImdb_v1\\\\aclImdb\\\\test\\\\neg\" \n",
    "\n",
    "list_of_train_pos_address = get_files_addresses(path_train_pos)  # list of address of positive training docs\n",
    "list_of_train_neg_address = get_files_addresses(path_train_neg)  # list of address of negative training docs\n",
    "list_of_test_pos_address = get_files_addresses(path_test_pos)  # list of address of positive test docs\n",
    "list_of_test_neg_address = get_files_addresses(path_test_neg)  # list of address of negative test docs\n",
    "\n",
    "# get the token list of each class\n",
    "pos_tokens = dataset_proccessing(list_of_train_pos_address)\n",
    "neg_tokens = dataset_proccessing(list_of_train_neg_address)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
