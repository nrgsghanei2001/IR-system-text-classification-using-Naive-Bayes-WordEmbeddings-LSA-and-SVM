{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing function:\n",
    "\n",
    "This function gets a list of texts and tokenize each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(list_of_contexts):\n",
    "    all_tokens = []         # a list to save all of tokens of all documents\n",
    "    for i, doc in enumerate(list_of_contexts):\n",
    "        lower_doc = doc.lower()              # make all of contexts lower case\n",
    "        list_of_contexts[i] = lower_doc          \n",
    "        tokens = re.findall(r'\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+', list_of_contexts[i])   # tokenize the text with regex\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    return all_tokens       # return a 2D array with contains lists of tokens of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering tokens:\n",
    "\n",
    "This function is for preprocessing tokens like removing stopwords and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def token_filtering(all_tokens):\n",
    "    for i, doc in enumerate(all_tokens):\n",
    "        new_tokens = []\n",
    "        for token in doc:     # delete all of stopwords and single character tokens except numbers from token list\n",
    "            if (len(token) < 2 and token.isalpha()) or (token in stop_words):          \n",
    "                continue\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        stemmer = nltk.stem.PorterStemmer()                      # stemming each token\n",
    "        new_tokens = [stemmer.stem(token) for token in new_tokens]\n",
    "        all_tokens[i] = new_tokens\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping list for terms to termIDs. each unique term is maped to a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_term_id(all_tokens):\n",
    "    termID = []\n",
    "    for doc in all_tokens:               # create a list of all unique terms in collection\n",
    "        for term in doc:\n",
    "            if term not in termID:    # if the term is not already in the list\n",
    "                termID.append(term)\n",
    "\n",
    "    return termID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D array(term-document matrix) that entry i and j is the TF of term i in document j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TF(tf, option, max_tf_d, max_tf_t):\n",
    "    if option==1:   # binary model {0, 1}\n",
    "        if tf>0:    \n",
    "            return 1\n",
    "        return 0\n",
    "    elif option==2:    # n: TF_t,d\n",
    "        return tf\n",
    "    elif option==3:    # l: 1+log(TF)\n",
    "        if tf>0:\n",
    "            return 1 + math.log2(tf)\n",
    "        return 0.1\n",
    "    elif option==4:     # m: ntf\n",
    "        return 0.4 + 0.6*((tf)/(max_tf_d))\n",
    "    else:               # a\n",
    "        return 0.5 + 0.5*((tf)/(max_tf_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_term_document_matrix(all_tokens, termID, option):\n",
    "    term_doc = np.zeros((len(termID), len(all_tokens)))  # an empy matrix rows=len(terms) and columns=len(collection)\n",
    "    # build term-doc matrix\n",
    "    for i, doc in enumerate(all_tokens):\n",
    "        for term in doc:\n",
    "            try:   # if term is happened in the termID map\n",
    "                term_ind = termID.index(term)     # row\n",
    "                doc_ind = i                       # column\n",
    "                term_doc[term_ind][doc_ind] += 1  # increase the frequency of the term in the document\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    max_tf_row = []\n",
    "    max_tf_col = []\n",
    "    for i in range(len(term_doc)):             # find TF_max in each document and TF_max for each term\n",
    "        max_tf_row.append(max(term_doc[i]))\n",
    "    for j in range(len(term_doc[0])):\n",
    "        col = []\n",
    "        for i in range(len(term_doc)):\n",
    "            col.append(term_doc[i][j])\n",
    "        max_tf_col.append(max(col))\n",
    "\n",
    "    for i in range(len(term_doc)):            # update term-doc weights based on given option\n",
    "        for j in range(len(term_doc[i])):\n",
    "            max_tf_t = max_tf_row[i]\n",
    "            max_tf_d = max_tf_col[j]\n",
    "            tf = term_doc[i][j]\n",
    "            term_doc[i][j] = calculate_TF(tf, option, max_tf_d, max_tf_t)\n",
    "        \n",
    "    return term_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an IDF list that stores document frequency of each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate idf based on given algorithm\n",
    "def calculate_IDF(N, DF_t, option):\n",
    "    if option == 1:  # n\n",
    "        return 1\n",
    "    elif option==2:   # t\n",
    "        return math.log2(N/DF_t)\n",
    "    else:   # p\n",
    "        return max(0, math.log2((N - DF_t)/(DF_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_idf(all_tokens, term_ID, option):\n",
    "    idf = np.zeros(len(term_ID))\n",
    "\n",
    "    for i, term in enumerate(term_ID):\n",
    "        count = 0\n",
    "        for doc in all_tokens:     # if term happened in document i, increase doc frequency of it\n",
    "            if term in doc:\n",
    "                count += 1\n",
    "\n",
    "        idf_t = calculate_IDF(len(all_tokens), count, option)   # calculate idf_t and put it in list of idfs\n",
    "        idf[i] = idf_t\n",
    "\n",
    "    return idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
